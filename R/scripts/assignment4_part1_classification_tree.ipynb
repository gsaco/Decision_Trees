{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Part 1: Predicting Heart Disease Using a Classification Tree (R)\n",
    "\n",
    "This notebook implements a classification tree model to predict whether a person is likely to have heart disease using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "column_names <- c('age', 'sex', 'cp', 'restbp', 'chol', 'fbs', 'restecg', \n",
    "                  'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'hd')\n",
    "\n",
    "df <- read.csv('../input/processed.cleveland.data', \n",
    "               header = FALSE,\n",
    "               col.names = column_names,\n",
    "               na.strings = '?')\n",
    "\n",
    "cat(\"Original dataset shape:\", dim(df), \"\\n\")\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "cat(\"Missing values per column:\\n\")\n",
    "colSums(is.na(df))\n",
    "\n",
    "# Remove missing values\n",
    "df <- na.omit(df)\n",
    "cat(\"\\nDataset shape after removing missing values:\", dim(df), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable y (1 if heart disease, 0 otherwise)\n",
    "df$y <- ifelse(df$hd > 0, 1, 0)\n",
    "cat(\"Distribution of target variable:\\n\")\n",
    "table(df$y)\n",
    "\n",
    "# Remove the original hd column\n",
    "df$hd <- NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to factors\n",
    "categorical_vars <- c('sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal')\n",
    "df[categorical_vars] <- lapply(df[categorical_vars], as.factor)\n",
    "\n",
    "# Convert y to factor for classification\n",
    "df$y <- as.factor(df$y)\n",
    "\n",
    "cat(\"Dataset structure:\\n\")\n",
    "str(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Analysis (8 points)\n",
    "\n",
    "### (1 point) Split data and plot classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "set.seed(123)\n",
    "train_index <- createDataPartition(df$y, p = 0.7, list = FALSE)\n",
    "train_data <- df[train_index, ]\n",
    "test_data <- df[-train_index, ]\n",
    "\n",
    "cat(\"Training set size:\", nrow(train_data), \"\\n\")\n",
    "cat(\"Test set size:\", nrow(test_data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification tree without pruning\n",
    "tree_model <- rpart(y ~ ., data = train_data, method = \"class\")\n",
    "\n",
    "# Plot the classification tree\n",
    "png('../output/classification_tree_before_pruning_R.png', width = 1200, height = 800)\n",
    "rpart.plot(tree_model, main = \"Classification Tree (Before Pruning)\",\n",
    "           extra = 104, box.palette = \"RdBu\", shadow.col = \"gray\")\n",
    "dev.off()\n",
    "\n",
    "# Display tree info\n",
    "cat(\"Tree complexity parameters:\\n\")\n",
    "printcp(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Plot confusion matrix and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "predictions <- predict(tree_model, test_data, type = \"class\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm <- confusionMatrix(predictions, test_data$y, \n",
    "                      dnn = c(\"Predicted\", \"Actual\"))\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_table <- as.data.frame(cm$table)\n",
    "colnames(cm_table) <- c(\"Predicted\", \"Actual\", \"Freq\")\n",
    "cm_table$Predicted <- ifelse(cm_table$Predicted == \"0\", \"Does not have HD\", \"Has HD\")\n",
    "cm_table$Actual <- ifelse(cm_table$Actual == \"0\", \"Does not have HD\", \"Has HD\")\n",
    "\n",
    "p <- ggplot(cm_table, aes(x = Predicted, y = Actual, fill = Freq)) +\n",
    "  geom_tile() +\n",
    "  geom_text(aes(label = Freq), size = 8) +\n",
    "  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n",
    "  labs(title = \"Confusion Matrix (Before Pruning)\",\n",
    "       x = \"Predicted Label\", y = \"True Label\") +\n",
    "  theme_minimal() +\n",
    "  theme(text = element_text(size = 14))\n",
    "\n",
    "ggsave('../output/confusion_matrix_before_pruning_R.png', p, width = 8, height = 6, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The confusion matrix shows the performance of our classification model.\n",
    "- True Negatives: correctly predicted individuals without heart disease\n",
    "- True Positives: correctly predicted individuals with heart disease\n",
    "- False Positives: incorrectly predicted as having heart disease\n",
    "- False Negatives: incorrectly predicted as not having heart disease (more concerning in medical diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5 points) Fix overfitting using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 50 values of alpha (cp in rpart) equally spaced on logarithmic scale\n",
    "alpha_values <- exp(seq(log(exp(-10)), log(0.05), length.out = 50))\n",
    "\n",
    "cat(\"Number of alpha values:\", length(alpha_values), \"\\n\")\n",
    "cat(\"Alpha range:\", min(alpha_values), \"to\", max(alpha_values), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 4-fold cross-validation for each alpha\n",
    "set.seed(123)\n",
    "mean_accuracies <- numeric(length(alpha_values))\n",
    "\n",
    "for (i in seq_along(alpha_values)) {\n",
    "  # Create folds\n",
    "  folds <- createFolds(train_data$y, k = 4)\n",
    "  accuracies <- numeric(4)\n",
    "  \n",
    "  for (j in 1:4) {\n",
    "    # Split into train and validation\n",
    "    val_idx <- folds[[j]]\n",
    "    train_cv <- train_data[-val_idx, ]\n",
    "    val_cv <- train_data[val_idx, ]\n",
    "    \n",
    "    # Train model with current alpha\n",
    "    model_cv <- rpart(y ~ ., data = train_cv, method = \"class\",\n",
    "                      control = rpart.control(cp = alpha_values[i]))\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    pred_cv <- predict(model_cv, val_cv, type = \"class\")\n",
    "    accuracies[j] <- mean(pred_cv == val_cv$y)\n",
    "  }\n",
    "  \n",
    "  mean_accuracies[i] <- mean(accuracies)\n",
    "}\n",
    "\n",
    "# Find optimal alpha\n",
    "optimal_idx <- which.max(mean_accuracies)\n",
    "optimal_alpha <- alpha_values[optimal_idx]\n",
    "optimal_accuracy <- mean_accuracies[optimal_idx]\n",
    "\n",
    "cat(\"Optimal alpha:\", optimal_alpha, \"\\n\")\n",
    "cat(\"Optimal cross-validation accuracy:\", optimal_accuracy, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5 points) Plot Inaccuracy Rate vs Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inaccuracy rate\n",
    "inaccuracy_rates <- 1 - mean_accuracies\n",
    "\n",
    "# Create data frame for plotting\n",
    "plot_df <- data.frame(alpha = alpha_values, inaccuracy = inaccuracy_rates)\n",
    "\n",
    "# Plot Inaccuracy Rate vs Alpha\n",
    "p <- ggplot(plot_df, aes(x = alpha, y = inaccuracy)) +\n",
    "  geom_line() +\n",
    "  geom_point(size = 2) +\n",
    "  geom_vline(xintercept = optimal_alpha, color = \"red\", linetype = \"dashed\",\n",
    "             size = 1) +\n",
    "  annotate(\"text\", x = optimal_alpha * 2, y = max(inaccuracy_rates) * 0.9,\n",
    "           label = paste(\"Optimal α =\", round(optimal_alpha, 6)), color = \"red\") +\n",
    "  scale_x_log10() +\n",
    "  labs(title = \"Inaccuracy Rate vs Alpha\",\n",
    "       x = \"Alpha (log scale)\",\n",
    "       y = \"Inaccuracy Rate (1 - Accuracy)\") +\n",
    "  theme_minimal() +\n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "ggsave('../output/inaccuracy_vs_alpha_R.png', p, width = 10, height = 6, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Plot pruned tree and confusion matrix with optimal alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification tree with optimal alpha\n",
    "tree_pruned <- rpart(y ~ ., data = train_data, method = \"class\",\n",
    "                     control = rpart.control(cp = optimal_alpha))\n",
    "\n",
    "# Plot the pruned classification tree\n",
    "png('../output/classification_tree_after_pruning_R.png', width = 1200, height = 800)\n",
    "rpart.plot(tree_pruned, \n",
    "           main = paste(\"Classification Tree (After Pruning with α =\", round(optimal_alpha, 6), \")\"),\n",
    "           extra = 104, box.palette = \"RdBu\", shadow.col = \"gray\")\n",
    "dev.off()\n",
    "\n",
    "cat(\"Pruned tree complexity parameters:\\n\")\n",
    "printcp(tree_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with pruned tree\n",
    "predictions_pruned <- predict(tree_pruned, test_data, type = \"class\")\n",
    "\n",
    "# Calculate confusion matrix for pruned tree\n",
    "cm_pruned <- confusionMatrix(predictions_pruned, test_data$y,\n",
    "                             dnn = c(\"Predicted\", \"Actual\"))\n",
    "print(cm_pruned)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_table_pruned <- as.data.frame(cm_pruned$table)\n",
    "colnames(cm_table_pruned) <- c(\"Predicted\", \"Actual\", \"Freq\")\n",
    "cm_table_pruned$Predicted <- ifelse(cm_table_pruned$Predicted == \"0\", \"Does not have HD\", \"Has HD\")\n",
    "cm_table_pruned$Actual <- ifelse(cm_table_pruned$Actual == \"0\", \"Does not have HD\", \"Has HD\")\n",
    "\n",
    "p <- ggplot(cm_table_pruned, aes(x = Predicted, y = Actual, fill = Freq)) +\n",
    "  geom_tile() +\n",
    "  geom_text(aes(label = Freq), size = 8) +\n",
    "  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n",
    "  labs(title = \"Confusion Matrix (After Pruning)\",\n",
    "       x = \"Predicted Label\", y = \"True Label\") +\n",
    "  theme_minimal() +\n",
    "  theme(text = element_text(size = 14))\n",
    "\n",
    "ggsave('../output/confusion_matrix_after_pruning_R.png', p, width = 8, height = 6, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "After pruning the decision tree using the optimal alpha value obtained through 4-fold cross-validation:\n",
    "\n",
    "1. **Tree Complexity:** The pruned tree is significantly simpler with fewer nodes and lower depth compared to the unpruned tree, reducing overfitting.\n",
    "\n",
    "2. **Model Performance:** The pruned model may show similar or slightly different accuracy compared to the unpruned model on the test set. The key benefit is better generalization.\n",
    "\n",
    "3. **Interpretability:** The simpler pruned tree is easier to interpret and explain, which is crucial in medical applications.\n",
    "\n",
    "4. **Cross-validation:** The use of cross-validation helps ensure that the selected alpha parameter leads to a model that generalizes well to unseen data.\n",
    "\n",
    "5. **Trade-off:** There's a trade-off between model complexity and performance. The optimal alpha balances this trade-off by preventing overfitting while maintaining good predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
