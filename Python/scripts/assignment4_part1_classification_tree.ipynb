{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Part 1: Predicting Heart Disease Using a Classification Tree\n",
    "\n",
    "This notebook implements a classification tree model to predict whether a person is likely to have heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 123\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "column_names = ['age', 'sex', 'cp', 'restbp', 'chol', 'fbs', 'restecg', \n",
    "                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'hd']\n",
    "\n",
    "df = pd.read_csv('../input/processed.cleveland.data', \n",
    "                 names=column_names, \n",
    "                 na_values='?')\n",
    "\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove missing values\n",
    "df = df.dropna()\n",
    "print(\"\\nDataset shape after removing missing values:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable y (1 if heart disease, 0 otherwise)\n",
    "# hd > 0 indicates heart disease\n",
    "y = (df['hd'] > 0).astype(int)\n",
    "print(\"Distribution of target variable:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Drop the original hd column from features\n",
    "X = df.drop('hd', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables to convert to dummy variables\n",
    "# Categorical variables: sex, cp, fbs, restecg, exang, slope, ca, thal\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "X = pd.get_dummies(X, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "print(\"Features after creating dummy variables:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nDataset shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Analysis (8 points)\n",
    "\n",
    "### (1 point) Split data and plot classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification tree without pruning\n",
    "clf = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the classification tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, \n",
    "          feature_names=X.columns, \n",
    "          class_names=['No HD', 'Has HD'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Classification Tree (Before Pruning)', fontsize=16)\n",
    "plt.savefig('../output/classification_tree_before_pruning.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Tree depth: {clf.get_depth()}\")\n",
    "print(f\"Number of leaves: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Plot confusion matrix and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Does not have HD\", \"Has HD\"],\n",
    "            yticklabels=[\"Does not have HD\", \"Has HD\"])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix (Before Pruning)')\n",
    "plt.savefig('../output/confusion_matrix_before_pruning.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0, 0]}\")\n",
    "print(f\"False Positives: {cm[0, 1]}\")\n",
    "print(f\"False Negatives: {cm[1, 0]}\")\n",
    "print(f\"True Positives: {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The confusion matrix shows the performance of our classification model.\n",
    "- True Negatives: correctly predicted individuals without heart disease\n",
    "- True Positives: correctly predicted individuals with heart disease\n",
    "- False Positives: incorrectly predicted as having heart disease\n",
    "- False Negatives: incorrectly predicted as not having heart disease (more concerning in medical diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5 points) Fix overfitting using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 50 values of alpha equally spaced on a logarithmic scale between e^-10 and 0.05\n",
    "alpha_values = np.logspace(np.log(np.exp(-10)), np.log(0.05), 50, base=np.e)\n",
    "\n",
    "print(f\"Number of alpha values: {len(alpha_values)}\")\n",
    "print(f\"Alpha range: {alpha_values.min():.10f} to {alpha_values.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 4-fold cross-validation for each alpha\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mean_accuracies = []\n",
    "std_accuracies = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    clf_temp = DecisionTreeClassifier(ccp_alpha=alpha, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(clf_temp, X_train, y_train, cv=4, scoring='accuracy')\n",
    "    mean_accuracies.append(scores.mean())\n",
    "    std_accuracies.append(scores.std())\n",
    "\n",
    "# Find optimal alpha (maximum mean accuracy)\n",
    "optimal_idx = np.argmax(mean_accuracies)\n",
    "optimal_alpha = alpha_values[optimal_idx]\n",
    "optimal_accuracy = mean_accuracies[optimal_idx]\n",
    "\n",
    "print(f\"Optimal alpha: {optimal_alpha:.10f}\")\n",
    "print(f\"Optimal cross-validation accuracy: {optimal_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5 points) Plot Inaccuracy Rate vs Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inaccuracy rate (1 - Accuracy)\n",
    "inaccuracy_rates = [1 - acc for acc in mean_accuracies]\n",
    "\n",
    "# Plot Inaccuracy Rate vs Alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_values, inaccuracy_rates, marker='o', markersize=3)\n",
    "plt.axvline(x=optimal_alpha, color='r', linestyle='--', \n",
    "            label=f'Optimal α = {optimal_alpha:.6f}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (log scale)', fontsize=12)\n",
    "plt.ylabel('Inaccuracy Rate (1 - Accuracy)', fontsize=12)\n",
    "plt.title('Inaccuracy Rate vs Alpha', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../output/inaccuracy_vs_alpha.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Plot pruned tree and confusion matrix with optimal alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification tree with optimal alpha\n",
    "clf_pruned = DecisionTreeClassifier(ccp_alpha=optimal_alpha, random_state=RANDOM_STATE)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Plot the pruned classification tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf_pruned, \n",
    "          feature_names=X.columns, \n",
    "          class_names=['No HD', 'Has HD'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f'Classification Tree (After Pruning with α = {optimal_alpha:.6f})', fontsize=16)\n",
    "plt.savefig('../output/classification_tree_after_pruning.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pruned tree depth: {clf_pruned.get_depth()}\")\n",
    "print(f\"Pruned tree number of leaves: {clf_pruned.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with pruned tree\n",
    "y_pred_pruned = clf_pruned.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix for pruned tree\n",
    "cm_pruned = confusion_matrix(y_test, y_pred_pruned)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_pruned, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Does not have HD\", \"Has HD\"],\n",
    "            yticklabels=[\"Does not have HD\", \"Has HD\"])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix (After Pruning)')\n",
    "plt.savefig('../output/confusion_matrix_after_pruning.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "print(f\"Accuracy on test set (pruned): {accuracy_pruned:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (Pruned):\")\n",
    "print(f\"True Negatives: {cm_pruned[0, 0]}\")\n",
    "print(f\"False Positives: {cm_pruned[0, 1]}\")\n",
    "print(f\"False Negatives: {cm_pruned[1, 0]}\")\n",
    "print(f\"True Positives: {cm_pruned[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "After pruning the decision tree using the optimal alpha value obtained through 4-fold cross-validation:\n",
    "\n",
    "1. **Tree Complexity:** The pruned tree is significantly simpler with fewer nodes and lower depth compared to the unpruned tree, reducing overfitting.\n",
    "\n",
    "2. **Model Performance:** The pruned model may show similar or slightly different accuracy compared to the unpruned model on the test set. The key benefit is better generalization.\n",
    "\n",
    "3. **Interpretability:** The simpler pruned tree is easier to interpret and explain, which is crucial in medical applications.\n",
    "\n",
    "4. **Cross-validation:** The use of cross-validation helps ensure that the selected alpha parameter leads to a model that generalizes well to unseen data.\n",
    "\n",
    "5. **Trade-off:** There's a trade-off between model complexity and performance. The optimal alpha balances this trade-off by preventing overfitting while maintaining good predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
